<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: fluentd | Glide Note]]></title>
  <link href="http://blog.glidenote.com/blog/categories/fluentd/atom.xml" rel="self"/>
  <link href="http://blog.glidenote.com/"/>
  <updated>2018-08-25T19:22:05+09:00</updated>
  <id>http://blog.glidenote.com/</id>
  <author>
    <name><![CDATA[Akira Maeda]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[CloudFrontのログをfluent-plugin-bigqueryを利用してBigQueryに入れるようにした]]></title>
    <link href="http://blog.glidenote.com/blog/2014/11/21/cloudfront-logs-to-bigquery/"/>
    <updated>2014-11-21T14:00:00+09:00</updated>
    <id>http://blog.glidenote.com/blog/2014/11/21/cloudfront-logs-to-bigquery</id>
    <content type="html"><![CDATA[<h2 id="tldr">TL;DR</h2>

<ul>
  <li>Amazon CloudFrontのアクセスログをBigQueryに入れるようにした</li>
  <li>BigQueryへのデータ投入には社内の他プロジェクトでも利用していて実績があり、KAIZENがメンテナになっている<a href="https://github.com/kaizenplatform/fluent-plugin-bigquery">fluent-plugin-bigquery</a>を利用</li>
</ul>

<h2 id="section">背景</h2>

<p>ここ2ヶ月くらい、あらゆるログをBigQueryに集約しつつあって、今回はAmazon CloudFrontのアクセスログについて作業をした。</p>

<p>Amazon CloudFrontのアクセスログには以下のような特徴がある。</p>

<ul>
  <li>Amazon CloudFrontのアクセスログは数時間〜1日程度遅れでS3のBucketに追加される。時系列はバラバラ</li>
  <li>CloudFrontのアクセスログがtsv形式。</li>
  <li>ベストエフォート型で全てのログがS3に収容される保証は無い</li>
  <li><code>gz</code>で圧縮されてS3に追加される(BigQueryに入れるにはgz形式から展開する必要がある)</li>
  <li>ログファイルの数が非常に多い。作業していた環境でだいたい1日に15,000個くらい<code>gz</code>ファイルがあって、全部をダウンロードするのに160分くらいかかる</li>
</ul>

<p>という感じ。</p>

<h2 id="section-1">仕組みの詳細</h2>

<p>データの流れ</p>

<p><img src="https://blog.glidenote.com/images/2014/11/cf2bq01.png" alt="" /></p>

<ol>
  <li>CloudFrontのログが自動でS3に追加される</li>
  <li>batchサーバから<code>s3cmd sync</code>でcronで定期的にs3からログファイル(<code>gz</code>形式)をローカルに持ってくる</li>
  <li>batchサーバで、取得したログファイルを<code>zcat ${FILE} &gt;&gt; /var/log/cloudfront/access.log</code> で連結する</li>
  <li>batchサーバのFluentdで<code>in_tail</code>を用い<code>/var/log/cloudfront/access.log</code>を読み込む</li>
  <li><code>fluent-plugin-bigquery</code>を用いてBigQueryにデータを投入する(日付毎にデータ投入先のテーブルを分ける)</li>
</ol>

<p>こんな感じでBigQueryにデータが入っている</p>

<p><img src="https://blog.glidenote.com/images/2014/11/cf2bq02.png" alt="" /></p>

<p>先日発表された<a href="https://blog.glidenote.com/blog/2014/11/20/s3-event-notificasions/">S3 Event Notificasionsを検証中</a>なので、問題が無ければ、Amazon SQSを利用した処理に切り替え予定。</p>

<h2 id="section-2">この方式を採用した理由</h2>

<ul>
  <li>私がFluentdの運用に慣れていた。</li>
  <li>KAIZENがメンテナをしているfluent-plugin-bigqueryを利用したかった</li>
  <li>BigQueryへのデータ投入部分を自前で実装しなくて良い</li>
  <li>BigQueryへのデータ投入周りのエラーハンドリング、再送処理をfluent-plugin-bigqueryに任すことが出来る</li>
  <li>logrotateを利用して、ログの世代管理が出来る。Fluentdがログ切り替えを検知出来る。(nginxのログなど他のアクセスログと同じような感じで扱える)</li>
  <li>Fluentdを利用しているので、スケールが容易</li>
  <li><code>/var/log/cloudfront/access.log-YYYYMMDD.gz</code>というように日別でログが分かれているので、障害やトラブル発生時のリカバリーも簡単になっている。(障害が発生した日の15,000個とかのログをS3から持ってきて云々… みたいな面倒な処理が必要ない)</li>
</ul>

<h2 id="section-3">その他</h2>

<ul>
  <li>CloudFrontのログがS3に追加されるのが遅い場合1日程度かかるので、翌日のテーブルにデータが入ってしまう。BigQueryからデータを取り出すときに工夫が必要</li>
  <li>BigQueryにデータ投入先のテーブルが存在しないと死ぬので、別でテーブル作成の仕組みと監視をしておく必要あり。</li>
  <li>データサイズが大きい場合、テーブルを適切に分割していないとBigQuery破産に。</li>
  <li>テーブルの自動作成、監視には<a href="https://rubygems.org/gems/bigquery">bigquery</a>を利用</li>
</ul>

<p>該当部分の<code>td-agent.conf</code>の設定</p>

<p>```</p>
<source />

<p>type         tail 
  path         /var/log/cloudfront/access.log
  pos_file     /var/log/td-agent/cloudfront-access-log.pos
  tag          extracted.cflog</p>

<p>format       tsv
  keys         day,time,x_edge_location,sc_bytes,c_ip,cs_method,cs_Host,cs_uri_stem,sc_status,cs_Referer,cs_User_Agent,cs_uri_query,cs_Cookie,x_edge_result_type,x_edge_request_id,x_host_header,cs_protocol,cs_bytes,time_take
&lt;/source&gt;</p>

<match extracted.cflog="">
  type                        bigquery

  buffer_type                 file
  buffer_path                 /var/log/td-agent/cf.bq.*.buffer

  method                      insert

  auth_method                 private_key
  email                       ''
  private_key_path            ''

  buffer_chunk_limit          768k
  buffer_queue_limit          5000
  flush_interval              1
  try_flush_interval          0.05 
  num_threads                 10 
  queued_chunk_flush_interval 0.01

  project                     ''
  dataset                     ''
  table                       %Y%m%d

  fetch_schema                true
</match>
<p>```</p>

<p>BigQueryにログを集約したことで集計も調査も非常に簡単になって、本当にBigQuery素晴らしい。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TAI64N形式のタイムスタンプをparseするFluentdのプラグインfluent-plugin-tai64n_parserを作った]]></title>
    <link href="http://blog.glidenote.com/blog/2014/02/13/fluent-plugin-tai64n_parser/"/>
    <updated>2014-02-13T20:16:00+09:00</updated>
    <id>http://blog.glidenote.com/blog/2014/02/13/fluent-plugin-tai64n_parser</id>
    <content type="html"><![CDATA[<p><strong>追記 2014年2月19日</strong>
本記事の初投稿後に修正がかなり入っているので、最新の利用方法については<a href="https://github.com/glidenote/fluent-plugin-tai64n_parser">glidenote/fluent-plugin-tai64n_parser</a>を参照ください。</p>

<ul>
  <li><a href="https://rubygems.org/gems/fluent-plugin-tai64n_parser">fluent-plugin-tai64n_parser | RubyGems.org | your community gem host</a></li>
  <li><a href="https://github.com/glidenote/fluent-plugin-tai64n_parser">glidenote/fluent-plugin-tai64n_parser</a></li>
</ul>

<p>TAI64N好きですか!!qmailのログとかでよく見る下記みたいなやつ。</p>

<p><code>
@4000000052fafd8d3298434c new msg 3890
@4000000052fafd8d32984b1c info msg 3890: bytes 372 from &lt;root@**********.pb&gt; qp 31835 uid 0
@4000000052fafd8d373b5dbc starting delivery 9: msg 3890 to remote glidenote@********.co.jp
@4000000052fafd8d373b6974 status: local 0/120 remote 1/60
@4000000052fafd8d38754cec delivery 9: success: ***.***.***.***_accepted_message./Remote_host_said:_250_ok_1392180611_qp_10394/
@4000000052fafd8d387554bc status: local 0/120 remote 0/60
@4000000052fafd8d387554bc end msg 3890
</code></p>

<p>私は大嫌いで見たくないです。見たくないけど残念ながら結構見かけるし、Fluentdにただ流してもナノ秒が処理が出来ないので
TAI64N形式のタイムスタンプをparseする<code>fluent-plugin-tai64n_parser</code>というものを作った。
既に本番サーバに投入して毎分30000メッセージくらい処理させてるけど、負荷もなく安定して動いてる。</p>

<h2 id="section">設定例</h2>

<p>```
&lt;match test.**&gt;
  type             tai64n_parser</p>

<p>key              tai64n
  output_key       parsed_time
  add_tag_prefix   parsed.
&lt;/match&gt;
```</p>

<p>下記みたいなメッセージが流れてくると</p>

<p><code>js
"test" =&gt; {
  "tai64n"      =&gt; "@4000000052f88ea32489532c"
}
</code></p>

<p>下記のように<code>tai64nlocal</code>かけたときと同じようにparseされる。</p>

<p><code>js
"parsed.test" =&gt; {
  "tai64n"      =&gt; "@4000000052f88ea32489532c"
  "parsed_time" =&gt; "2014-02-10 17:32:25.612979500",
}
</code></p>

<h2 id="qmail">qmailの送信ログを処理してみる</h2>

<p>たとえばqmailの送信ログを処理する場合は、下記のような感じで設定</p>

<p>```
<match raw.qmail.sent="">
  type             parser
  remove_prefix    raw
  format           /^(?<tai64n>[^ ]+) (?<message>(((?:new|end) msg (?<key>[0-9]+)|info msg (?<key>[0-9]+): bytes (?:\d+) from &lt;(?&lt;address&gt;[^&gt;]*)&gt; |starting delivery (?<delivery_id>[0-9]+): msg (?<key>[0-9]+) to (?:local|remote) (?&lt;address&gt;.+)|delivery (?<delivery_id>[0-9]+))?.*))$/
  key_name         message
  suppress_parse_error_log true
&lt;/match&gt;</delivery_id></key></delivery_id></key></key></message></tai64n></match></p>

<match qmail.sent="">
  type             tai64n_parser

  key              tai64n
  output_key       parsed_time
  add_tag_prefix   parsed.
</match>

<match parsed.qmail.sent="">
  type file
  path /var/log/td-agent/qmail_tai64n_parsed.log
</match>
<p>```</p>

<p>すると下記みたいな感じでパースされて、ナノ秒単位で<code>parsed_time</code>に変換された時間が入ってくる。</p>

<p><code>
2014-02-12T13:50:11+09:00       parsed.qmail.sent       {"tai64n":"@4000000052fafd8d3298434c","message":"new msg 3890","key":"3890","parsed_time":"2014-02-12 13:50:11.848839500"}
2014-02-12T13:50:11+09:00       parsed.qmail.sent       {"tai64n":"@4000000052fafd8d32984b1c","message":"info msg 3890: bytes 372 from &lt;root@**********.pb&gt; qp 31835 uid 0","key":"3890","address":"root@**********.pb","parsed_time":"2014-02-12 13:50:11.848841500"}
2014-02-12T13:50:11+09:00       parsed.qmail.sent       {"tai64n":"@4000000052fafd8d373b5dbc","message":"starting delivery 9: msg 3890 to remote glidenote@**********.co.jp","key":"3890","address":"glidenote@**********.co.jp","delivery_id":"9","parsed_time":"2014-02-12 13:50:11.926637500"}
2014-02-12T13:50:11+09:00       parsed.qmail.sent       {"tai64n":"@4000000052fafd8d373b6974","message":"status: local 0/120 remote 1/60","parsed_time":"2014-02-12 13:50:11.926640500"}
2014-02-12T13:50:11+09:00       parsed.qmail.sent       {"tai64n":"@4000000052fafd8d38754cec","message":"delivery 9: success: ***.***.***.***_accepted_message./Remote_host_said:_250_ok_1392180611_qp_10394/","delivery_id":"9","parsed_time":"2014-02-12 13:50:11.947211500"}
2014-02-12T13:50:11+09:00       parsed.qmail.sent       {"tai64n":"@4000000052fafd8d387554bc","message":"status: local 0/120 remote 0/60","parsed_time":"2014-02-12 13:50:11.947213500"}
2014-02-12T13:50:11+09:00       parsed.qmail.sent       {"tai64n":"@4000000052fafd8d387554bc","message":"end msg 3890","key":"3890","parsed_time":"2014-02-12 13:50:11.947213500"}
</code></p>

<p>上の例だと<code>fluent-plugin-tai64n_parser</code>でparse後に、<a href="https://github.com/sonots/fluent-plugin-record-reformer">sonots/fluent-plugin-record-reformer</a>のようなプラグインを使って
<code>tai64n</code>自体のキーを消して、DBに入れればTAI64N形式のタイムスタンプを二度と見なくて良くなる。</p>

<h2 id="section-1">参考</h2>

<ul>
  <li><a href="https://github.com/CaDs/fluent-plugin-time_parser/">CaDs/fluent-plugin-time_parser</a></li>
  <li><a href="https://gist.github.com/hayajo/2848763">fluentd qmail-send log format</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[td-agentでqueue size exceeds limit]]></title>
    <link href="http://blog.glidenote.com/blog/2014/01/28/td-agent-queue-size-exceeds-limit/"/>
    <updated>2014-01-28T19:58:00+09:00</updated>
    <id>http://blog.glidenote.com/blog/2014/01/28/td-agent-queue-size-exceeds-limit</id>
    <content type="html"><![CDATA[<p>昨年構築してからずっと安定稼働していたfluend(td-agent)が年明けくらいから下記のエラーが出て連日停止してしまって、
ちょっとハマったのでメモしておく。(ハッキリとした原因は掴めてないですが)</p>

<p>利用しているtd-agentのバージョンは<code>td-agent-1.1.18-0.x86_64</code>。</p>

<h2 id="section">現象</h2>

<p>fluent.logには下記が出ていて、機能が停止している。</p>

<p><code>
2014-01-09T20:28:59+09:00       fluent.error    {"error":"#&lt;Fluent::BufferQueueLimitError: queue size exceeds limit&gt;","error_class":"Fluent::BufferQueueLimitError","message":"forward error"}
2014-01-09T20:28:59+09:00       fluent.warn     {"error_class":"Fluent::BufferQueueLimitError","error":"#&lt;Fluent::BufferQueueLimitError: queue size exceeds limit&gt;","message":"emit transaction failed "}
2014-01-09T20:28:59+09:00       fluent.warn     {"error_class":"Fluent::BufferQueueLimitError","error":"#&lt;Fluent::BufferQueueLimitError: queue size exceeds limit&gt;","message":"emit transaction failed "}
2014-01-09T20:28:59+09:00       fluent.warn     {"error_class":"Fluent::BufferQueueLimitError","error":"#&lt;Fluent::BufferQueueLimitError: queue size exceeds limit&gt;","message":"emit transaction failed "}
2014-01-09T20:28:59+09:00       fluent.warn     {"error_class":"Fluent::BufferQueueLimitError","error":"#&lt;Fluent::BufferQueueLimitError: queue size exceeds limit&gt;","message":"emit transaction failed "}
2014-01-09T20:28:59+09:00       fluent.warn     {"error_class":"Fluent::BufferQueueLimitError","error":"#&lt;Fluent::BufferQueueLimitError: queue size exceeds limit&gt;","message":"emit transaction failed "}
</code></p>

<p>dmesgには下記が延々と出続けている。<code>possible SYN flooding on port 24224</code>はtd-agentがポート開いてるけどaccept(2)してないから出ていると@hiboma先生がサクッと検証用のコードを書いて調査してくれた。</p>

<p><code>
possible SYN flooding on port 24224. Sending cookies.
possible SYN flooding on port 24224. Sending cookies.
possible SYN flooding on port 24224. Sending cookies.
</code></p>

<p>構成は<a href="https://blog.glidenote.com/blog/2013/04/08/fluentd-mongodb-integer/">以前書いたもの</a>のまま。</p>

<p><img src="https://blog.glidenote.com/images/2013/04/fluentd-mongo1.png" alt="" /></p>

<h2 id="section-1">対処方法</h2>

<p>いろいろ調査して、試してみたんですが、先に対処内容を書いておくと、@hiboma先生が調べてくれた
<a href="http://buta9999.hatenablog.jp/entry/20131120/1384958564">fluentdで死の宣告queue size exceeds limit - boku no blog</a>
を参考に、<code>buffer_type</code>を<code>memory</code>から<code>file</code>に変更して現象は収まって復旧。</p>

<p>変更内容は下記のような感じ</p>

<p><div><script src='https://gist.github.com/8469146.js'></script>
<noscript><pre><code>&lt;html&gt;&lt;body&gt;You are being &lt;a href=&quot;https://github.com/gist/8469146&quot;&gt;redirected&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;</code></pre></noscript></div>
</p>

<p><code>buffer_chunk_limit</code>と<code>buffer_queue_limit</code>の値は結構気をつけてたんですが、
メモリ64GB+SSDのサーバで、盲目的に<code>buffer_type memory</code>にしていたのが災いした模様。</p>

<h2 id="section-2">調査でやったこと</h2>

<ul>
  <li><code>buffer_chunk_limit</code>,<code>buffer_queue_limit</code>の調整</li>
  <li>strace、gdbでバックトレース</li>
  <li>monitoring agentを利用して詳細リソースの監視</li>
  <li><code>kill -USR1</code> 飛ばしてみる</li>
</ul>

<p>など、@hibomaと@hfmに協力してもらってかなりいろいろなことを試してみたけど、症状も改善せず。</p>

<p>調査で分かったのは<code>queue size exceeds limit</code>が出て、いきなり止まるんじゃなくて、logrotateのタイミングで死んでるような挙動。
logrotate内で実行している<code>kill -USR1</code>を実行してみると旧プロセスが残ったままになったりして、調査している問題と別の問題と思われる謎の挙動が発生して
訳が分からない状態に…</p>

<h2 id="section-3">参考</h2>

<ul>
  <li><a href="http://buta9999.hatenablog.jp/entry/20131120/1384958564">fluentdで死の宣告queue size exceeds limit - boku no blog</a></li>
  <li><a href="http://d.hatena.ne.jp/wyukawa/20130212/1360657461">fluentdの複数実行 - wyukawa’s blog</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[FluentdからMongoDBにデータを文字列(string)から数値(integer)に変換して入れる]]></title>
    <link href="http://blog.glidenote.com/blog/2013/04/08/fluentd-mongodb-integer/"/>
    <updated>2013-04-08T23:00:00+09:00</updated>
    <id>http://blog.glidenote.com/blog/2013/04/08/fluentd-mongodb-integer</id>
    <content type="html"><![CDATA[<p>最近ようやく仕事でガッツリFluentdとMongoDBを触るようになってちょっとハマったことが
あったので誰かの役に立つかも知れないので書いておく。</p>

<p>Fluentd界隈は開発が活発なので、数ヶ月後にはこの記事の情報も恐らく古くなっているのでご注意ください。</p>

<h2 id="section">結論</h2>

<p>書いてたら長くなってしまったので先に要点というか結論を書いておく。</p>

<ul>
  <li><code>apache</code> =&gt; <code>fluentd</code> =&gt; <code>MongoDB</code> とapacheのアクセスログをMongoDBに入れたときに<code>code</code>や<code>size</code>といった数値が文字列(string)として入っていた</li>
  <li><code>td-agent.conf</code>の<code>format</code>を自作すると、時間以外のフィールドが文字列(string)として扱われる</li>
  <li><a href="https://rubygems.org/gems/fluent-plugin-typecast">fluent-plugin-typecast</a>を利用することで<code>code</code>や<code>size</code>といった特定のフィールドを、<code>string</code>や<code>integer</code>などの任意のデータ型に変換可能。今回の場合は<code>size</code>と<code>code</code>をintegerとして扱うように変換して解決した</li>
  <li>MongoDBのAggregation Frameworkは大変便利</li>
</ul>

<h2 id="section-1">前段</h2>

<p>MongoDBは2.2から<a href="http://docs.mongodb.org/manual/aggregation/">Aggregation Framework</a>というのが
使えるようになっていて、これまでMapReduceなどを用いないと実行出来なかったデータの集計作業などが
コマンド一発で出来るようになっているとのこと。
Fluentdを利用してMongoDBに貯めていたapacheのログからvhost毎のトラフィック集計作業を行おうとしてた。</p>

<p>MongoDBに入っているapacheのログデータは下記のような形</p>

<p><code>json
{ 
  "_id" : ObjectId("515e912431166b166b000002"), 
  "host" : "hogemoge.glidenote.com", 
  "user" : "192.168.25.84", 
  "method" : "GET", 
  "path" : "/htdocs_error_Zq9kbQHobRDu8hdp4K06lMGUOLwFoY0dQUSsIqgXLVBYB3gwAIBy9NNcd9coPHRV/css/error.css", 
  "code" : "200", 
  "size" : "423", 
  "referer" : "http://hogemoge.glidenote.com/", 
  "agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_5) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.43 Safari/537.31", "time" : ISODate("2013-04-05T08:53:50Z") 
}
</code></p>

<p><code>hogemoge.glidenote.com</code>の転送量の集計をするために下記コマンドで<code>size</code>で集計をかける。</p>

<p><code>json
db.accesslog.aggregate(
  {$match   :{"vhost":"hogemoge.glidenote.com" } },
  {$project :{"vhost":1, "size":1} },
  {$group   :{"_id":"$vhost", "traffic":{"$sum":"$size"} } }
);
</code></p>

<p>…、集計出来ないッ!</p>

<h2 id="aggregation-framework">aggregation frameworkで集計出来ない原因</h2>

<p>なぜ集計出来ないのか調査すると、<code>size</code>が文字列として扱われていた。ちなみに各フィールドのデータ型は下記のような形で調べられる。(ダブルクオートで囲まれているので、その時点でintegerではないんですが調査中は全く気づかなかった…)</p>

<p>``` json
// type 2で抽出されるものはstring
db.accesslog.find( { size : { $type : 2 } } )</p>

<p>// type 16で抽出されるものは32-bit integer
db.accesslog.find( { size : { $type : 16 } } )
```</p>

<h2 id="section-2">数値が文字列として扱われている問題の原因</h2>

<p>同じような現象を調べて下記有益情報を発見。</p>

<ul>
  <li><a href="http://d.hatena.ne.jp/hetaremania/20120627/1340789087">fluentdからmongodbにint（数値）として突っ込む - おどろく程やる気のない日々</a></li>
  <li><a href="http://d.hatena.ne.jp/mikeda/20120514/1337010657">fluentdのin_tailで特定フィールドをintに変えたい - IT 東京 楽しいと思うこと</a></li>
</ul>

<h3 id="section-3">問題が発生した時の構成</h3>

<p>問題が発生していたときの構成。(WEBサーバ約200台で、LOGサーバは1台の構成)</p>

<p><img src="https://blog.glidenote.com/images/2013/04/fluentd-mongo0.png" alt="" /></p>

<p>結論を言うと今回の現象はLOGサーバ側で利用していた<code>fluent-plugin-parser</code>の<a href="https://github.com/tagomoris/fluent-plugin-parser/blob/master/lib/fluent/plugin/fixed_parser.rb">fixed_parser.rb</a>部分で発生していた。<a href="https://github.com/fluent/fluentd/blob/master/lib/fluent/parser.rb">fluentdのparser.rb</a>を利用しても同様に発生する問題(2013年4月8日現在)  </p>

<h3 id="string">なぜ数値が文字列(string)として扱われるのか</h3>

<p>解決方法を探るためにソースを見てみると、<code>format apache2</code>の場合は<code>ApacheParser</code>が利用されて<code>size</code>や<code>code</code>に<code>to_i</code>の処理が入っており数値に変換されているが、<code>format</code>を自分で書くと<code>RegexpParser</code>が利用されて<code>time</code>以外は文字列として扱われている。今回は<code>format apache2</code>がそのまま利用出来ないログの形式でformatを自分で書いていたのも一因としてある。</p>

<p><a href="https://github.com/tagomoris/fluent-plugin-parser/blob/master/lib/fluent/plugin/fixed_parser.rb#L178-L187">https://github.com/tagomoris/fluent-plugin-parser/blob/master/lib/fluent/plugin/fixed_parser.rb#L178-L187</a></p>

<h2 id="section-4">問題の解決方法</h2>

<p>問題の原因がわかり、解決方法をいろいろ試していたら、弊社の<a href="https://twitter.com/banyan">@banyan</a>先生から<a href="https://github.com/tarom/fluent-plugin-typecast">fluent-plugin-typecast</a>というのがあると有益情報をゲット。</p>

<p>早速下記のような形で導入</p>

<p><code>sh
/usr/lib64/fluent/ruby/bin/fluent-gem install fluent-plugin-typecast
</code></p>

<p><code>td-agent.conf</code>の<code>fluent-plugin-typecast</code>の部分は下記のような形</p>

<p><code>
&lt;match get.apache.access&gt;
  type typecast
  item_types size:integer,code:integer
  tag filtered.apache.access
&lt;/match&gt;
</code></p>

<p>apacheのログを流してみると、ちゃんと<code>size</code>と<code>code</code>が数値(integer)としてMongoDBに入るようになった。</p>

<h2 id="aggregation-framework-1">Aggregation Frameworkで再度集計してみる</h2>

<p>当初の目的であるMongoDBのAggregation Frameworkを再度試してみる。</p>

<p>``` json
db.accesslog.aggregate(
  {$match   :{“vhost”:”hogemoge.glidenote.com” } },
  {$project :{“vhost”:1, “size”:1} },
  {$group   :{“_id”:”$vhost”, “traffic”:{“$sum”:”$size”} } }
);</p>

<p>{
    “result” : [
        {
            “_id” : “hogemoge.glidenote.com”,
            “traffic” : 289736716
        }
    ],
    “ok” : 1
}
```</p>

<p>一瞬で集計が完了。素晴らしい!!(<code>$match</code>を書けるvhost、timeとかにindex貼ってないと多分死にます)</p>

<h3 id="section-5">改善後の構成</h3>

<p><img src="https://blog.glidenote.com/images/2013/04/fluentd-mongo1.png" alt="" /></p>

<p>この構成で1秒に約4000ドキュメント、1日で約3億5千ドキュメントをMongoDBに突っ込んで
数日様子を見てますが、特に大きな問題も無く稼働中です。</p>

<p>当初どこが問題の原因か全く検討も付かなくて調査に半日近く時間を要した。
仕事でガッツリ触らないと覚えないし、勘も働かないなーと改めて感じた。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[fluentdのformat(正規表現)の作り方について試行錯誤中 #fluentd]]></title>
    <link href="http://blog.glidenote.com/blog/2012/07/15/fluentd-regex-debug/"/>
    <updated>2012-07-15T23:59:00+09:00</updated>
    <id>http://blog.glidenote.com/blog/2012/07/15/fluentd-regex-debug</id>
    <content type="html"><![CDATA[<p><a href="http://fluentd.org/">Fluentd</a>を触るようになって、いろんなログをfluentdに
渡すように試行錯誤している最中。</p>

<p><code>td-agent.conf</code>、<code>fluent.conf</code>を用意するときに任意のjson形式にするために
正規表現を用いてformatを書く必要があるんですが、formatの作り方というかデバック方法について
どういう手順に作ると良いのか情報がネット上に見当たらず試行錯誤中。  </p>

<p>もっと良い方法を教えてもらいたいので、今やっている方法を晒してみる。</p>

<h2 id="format">そもそもの疑問、どうやってformatを作るのか</h2>

<p>たとえばfluentd関連の情報を調べてると、</p>

<ul>
  <li><a href="http://d.hatena.ne.jp/sfujiwara/20120326/1332760934">#fluentd で maillog を読み込んで MongoDB に投入 - 酒日記 はてな支店</a></li>
</ul>

<p><code>
format /^(?&lt;date&gt;[^ ]+) (?&lt;host&gt;[^ ]+) (?&lt;process&gt;[^:]+): (?&lt;message&gt;((?&lt;key&gt;[^ :]+)[ :])? ?((to|from)=&lt;(?&lt;address&gt;[^&gt;]+)&gt;)?.*)$/
</code></p>

<ul>
  <li><a href="http://blog.kentarok.org/entry/2012/07/01/000518">fluent-plugin-rewriteというプラグインを作成した #fluentd - delirious thoughts</a></li>
</ul>

<p><code>
format /^[^ ]+ [^ ]+ [^ ]+ [^ ]+ \[(?&lt;time&gt;[^\]]+)\] "(?&lt;method&gt;[^ ]+) (?&lt;path&gt;[^ ]+) [^"]+" (?&lt;status&gt;[^ ]+) [^ ]+ [^ ]+ "[^"]+" "[^"]+" (?&lt;response_time&gt;[^ ]+)$/
</code></p>

<p>という感じで、formatの正規表現がサラッと書いてあって、
当然作れるでしょ的な感じでformatの作り方については言及されてない。
でも実際に作ろうとすると、どういう方法でやればいいのかよく分からない。</p>

<h2 id="squidaccesslogjson">やりたいこと、squidのaccess.logをjson形式に</h2>

<p>たとえば下記のようなsquidのaccess.log(<a href="http://wiki.squid-cache.org/Features/LogFormat">squidのログ形式</a>)をfluentdに渡して</p>

<p><code>
1342297357.149    440 192.168.11.3 TCP_MISS/302 627 GET http://glidenote.disqus.com/count.js - HIER_DIRECT/75.126.109.204 text/javascript
</code></p>

<p>下記のようにシリアライズしたい。</p>

<p><code>
{
  "date"           : "1342297357.149",
  "duration"       : "440",
  "client address" : "192.168.11.3",
  "result code"    : "TCP_MISS/302",
  "bytes"          : "627",
  "request method" : "GET",
  "url"            : "http://glidenote.disqus.com/count.js",
  "rfc931"         : "-",
  "hierarchy code" : "HIER_DIRECT/75.126.109.204",
  "type"           : "text/javascript"
}
</code></p>

<p>でもこれを実現するための<code>format</code>の作り方の手順を解説しているところが無い。(調べ方が悪いだけかもしれないですが）</p>

<h2 id="format-1">最初にformat作成のためにやっていたこと(非効率な方法)</h2>

<p>最初は</p>

<ol>
  <li>confを修正</li>
  <li>設定ファイル再読込</li>
  <li>確認</li>
</ol>

<p>というのを繰り返しやっていたんですが、非効率過ぎてこれだと<code>format</code>を作る気にならず途中で断念。
というのが数ヶ月前の話。</p>

<h2 id="format-2">今回format作成のためにやってみたこと</h2>

<p>今回再チャレンジでいろいろ調べてみると有益な情報が。</p>

<ul>
  <li><a href="http://oboerutech.blog16.fc2.com/blog-entry-49.html">覚えるテク [Ruby][Fluentd]in_tailの正規表現をテスト</a></li>
</ul>

<p><code>fluent/parserの"RegexpParser"クラス</code>を利用すれば<code>format</code>作成のデバッグが出来るらしい。</p>

<h3 id="section">デバッグ用スクリプトの用意</h3>

<p>上記サイトを参考に、下記のようなデバッグ用スクリプトを用意して<code>log</code>,<code>format</code>,<code>time_format</code>の部分を調整していく。
vim+quickrunとかの環境だと書きながらテストが出来るのでオススメです。
(外部ファイルを読み込ませる形より、<code>log</code>,<code>format</code>を直接修正しながらテストしたほうが効率的でした)</p>

<p><div><script src='https://gist.github.com/3113323.js'></script>
<noscript><pre><code>#!/usr/bin/env ruby
# -*- coding: utf-8 -*-

require &#39;time&#39;
require &#39;fluent/log&#39;
require &#39;fluent/config&#39;
require &#39;fluent/engine&#39;
require &#39;fluent/parser&#39;

$log ||= Fluent::Log.new

# debug
log = &#39;&#39;
format = //
time_format = &#39;&#39;

parser = Fluent::TextParser::RegexpParser.new(format, &#39;time_format&#39; =&gt; time_format)
puts parser.call(log)</code></pre></noscript></div>
</p>

<p>いきなり全部のformatを書くのは無理なので、下記のような感じでログの一部ずつやっていく。
（今回の例だと<code>time_format</code>は利用していないので空のままです)</p>

<p>下記のような形で<code>date</code>の項目だけ記載してシリアライズしてみる</p>

<p><div><script src='https://gist.github.com/3115861.js?file=format_debug1.rb'></script>
<noscript><pre><code>log = &#39;1342297357.149&#39;
format = /^(?&lt;date&gt;[^ ]+)$/
time_format = &#39;&#39;</code></pre></noscript></div>
</p>

<p>意図した形式で出力された。</p>

<p><code>
{"date"=&gt;"1342297357.149"}
</code></p>

<p>上手くいったら、<code>duration</code>など1個ずつ項目を増やしてやっていく。</p>

<p><div><script src='https://gist.github.com/3115861.js?file=format_debug2.rb'></script>
<noscript><pre><code>log = &#39;1342297357.149    440&#39; 
format = /^(?&lt;date&gt;[^ ]+)\s*(?&lt;duration&gt;.*)$/
time_format = &#39;&#39;</code></pre></noscript></div>
</p>

<p>これを繰り返していくと、下記のような形で出来上がる。</p>

<p><div><script src='https://gist.github.com/3115861.js?file=format_debug3.rb'></script>
<noscript><pre><code>log = &#39;1342297357.149    440 192.168.11.3 TCP_MISS/302 627 GET http://glidenote.disqus.com/count.js - HIER_DIRECT/75.126.109.204 text/javascript&#39;
format = /^(?&lt;date&gt;[^ ]+)\s*(?&lt;duration&gt;.*) (?&lt;client address&gt;.*) (?&lt;result code&gt;.*) (?&lt;bytes&gt;.*) (?&lt;request method&gt;.*) (?&lt;url&gt;.*) (?&lt;rfc931&gt;.*) (?&lt;hierarchy code&gt;.*) (?&lt;type&gt;.*)$/</code></pre></noscript></div>
</p>

<p>これを<code>conf</code>に反映。とりあえずfile書き出しで様子見。(td-agent.confには<code>=</code>は必要無いので消す)</p>

<p>```</p>
<source />

<p>type tail
  path /var/log/squid/access.log
  tag squid
  format /^(?<date>[^ ]+)\s+(?<duration>.*) (?<client address="">.*) (?<result code="">.*) (?<bytes>.*) (?<request method="">.*) (?<url>.*) (?<rfc931>.*) (?<hierarchy code="">.*) (?</hierarchy></rfc931></url></request></bytes></result></client></duration></date></p>
<type>.*)$/
  pos_file /var/log/fluent/squid.pos


&lt;match squid.**&gt;
  type file
  path /var/log/fluent/squid.log

```

これでやりたいことが実現出来た。

![](https://blog.glidenote.com/images/2012/07/fluentd-squid.png)

もっと上手いやり方ありそうなんだけど、「こうすると良い」とか誰かもっと良い方法を教えて欲しい。

**追記 2012年7月16日**

[@tagomoris](https://twitter.com/tagomoris/)さんがirbを利用する方法を教えてくれました。感謝です

 * [Fluentdでparser用の正規表現を書く・試す - tagomorisのメモ置き場](http://d.hatena.ne.jp/tagomoris/20120715/1342368392)

**追記 2012年10月01日**

今だとこういう便利なものがあるようです!!

 * [Fluentular: a Fluentd regular expression editor](http://fluentular.herokuapp.com/)

## 参考

 * [覚えるテク \[Ruby\]\[Fluentd\]in_tailの正規表現をテスト](http://oboerutech.blog16.fc2.com/blog-entry-49.html)
 
</type>
]]></content>
  </entry>
  
</feed>
